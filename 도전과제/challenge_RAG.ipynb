{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch3 LLM, RAG 개인 도전과제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal: LangSmith의 Prompt Library를 참고하여 prompt engineering을 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 사용환경 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"  # 라이브러리끼리의 충돌을 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. 모델 로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 모델 초기화\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. 문서 로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF 파일 로드. 파일의 경로 입력\n",
    "loader = PyPDFLoader(\"files/인공지능산업최신동향_2024년_11월호.pdf\")\n",
    "\n",
    "# 페이지 별 문서 로드\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. 문서 청크로 나누기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RecursiveCharacterTextSplitter**\n",
    "- 주어진 문자 목록의 순서대로 청크가 충분히 작아질 때까지 재귀적으로 텍스트를 분할하는 클래스\n",
    "- 문자 목록을 매개변수로 받아 동작\n",
    "- 기본 문자 목록: \\[\"\\n\\n\", \"\\n\", \" \", \"\"\\] (default)\n",
    "- 텍스트를 재귀적으로 분할하여 의미적으로 관련있는 텍스트 조각들이 같이 있도록 하는 목적으로 설계됨\n",
    "\n",
    "---\n",
    "- `chunk_size`: 각 청크의 최대 길이\n",
    "- `chunk_overlap`: 인접한 청크 사이에 중복으로 포함될 문자의 수\n",
    "- `length_function`: 청크의 길이를 계산하는 함수\n",
    "- `is_separator_regex`: 구분자로 정규식을 사용할지 여부를 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "splits = recursive_text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. 벡터 임베딩 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# OpenAI 임베딩 모델 초기화\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. 벡터 스토어 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7. FAISS를 Retriever로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. 프롬프트 템플릿 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3_envs\\SpartaProjects\\Personal_Project\\project2\\Lib\\site-packages\\langsmith\\client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "d:\\Anaconda3_envs\\SpartaProjects\\Personal_Project\\project2\\Lib\\site-packages\\langsmith\\client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "d:\\Anaconda3_envs\\SpartaProjects\\Personal_Project\\project2\\Lib\\site-packages\\langsmith\\client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# LangChain library에서 prompt, model, chain, agent 등을 불러오는 라이브러리\n",
    "from langchain import hub\n",
    "\n",
    "# Prompts 디렉토리가 없다면 Prompts 디렉토리 만들기\n",
    "os.makedirs(\"Prompts\", exist_ok=True)\n",
    "\n",
    "# hub.pull() 함수로 LangChain library에서 prompt 가져오기\n",
    "prompt1 = hub.pull(\"rlm/rag-prompt\")\n",
    "prompt2 = hub.pull(\"rlm/rag-document-relevance\")\n",
    "prompt3 = hub.pull(\"rlm/rag-answer-hallucination\")\n",
    "\n",
    "# 가져온 prompt들을 prompts list에 저장\n",
    "prompts = [prompt1, prompt2, prompt3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts 디렉토리에 prompt 저장\n",
    "for i in range(1, 4):\n",
    "    # 파일 경로를 .\\Prompts\\prompt.txt 형태로 설정\n",
    "    file_path = os.path.join(\".\\Prompts\", f\"prompt{i}.txt\")\n",
    "    \n",
    "    # format()으로 ChatPromptTemplate을 str 타입으로 변환\n",
    "    prompt = prompts[i-1].format(question=\"{question}\", context=\"{context}\")\n",
    "    \n",
    "    # 설정한 파일 경로로 변환한 prompt.txt 저장\n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt1': \"Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\", 'prompt2': 'System: You are a grader assessing relevance of a retrieved document to a user question.\\n\\nIf the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\\\n\\n\\nIt does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\\\n\\n\\nGive a binary score 1 or 0 score, where 1 means that the document is relevant to the question.\\nHuman: Retrieved documents:  \\n\\nUser question: ', 'prompt3': 'System: You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n\\nGive a binary score 1 or 0, where 1 means that the answer is grounded in / supported by the set of facts.\\nHuman: Facts:  \\n\\nLLM generation: '}\n"
     ]
    }
   ],
   "source": [
    "prompts = {}\n",
    "for i in range(1, 4):\n",
    "    file_path = os.path.join(\".\\Prompts\", f\"prompt{i}.txt\")\n",
    "    with open(file_path, 'r') as f:\n",
    "        prompts[f\"prompt{i}\"] = f.read()\n",
    "        \n",
    "print(prompts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=[] input_types={} partial_variables={} template='System: You are a grader assessing relevance of a retrieved document to a user question.\\n\\nIf the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\\\n\\n\\nIt does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\\\n\\n\\nGive a binary score 1 or 0 score, where 1 means that the document is relevant to the question.\\nHuman: Retrieved documents:  \\n\\nUser question: '\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = prompts[\"prompt2\"]\n",
    "prompt_test = PromptTemplate.from_template(prompt_template)\n",
    "print(prompt_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. RAG 체인 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "class DebugPassThrough(RunnablePassthrough):\n",
    "    def invoke(self, *args, **kwargs):\n",
    "        output = super().invoke(*args, **kwargs)\n",
    "        print(\"Debug Output:\", output)\n",
    "        return output\n",
    "# 문서 리스트를 텍스트로 변환하는 단계 추가\n",
    "class ContextToText(RunnablePassthrough):\n",
    "    def invoke(self, inputs, config=None, **kwargs):  # config 인수 추가\n",
    "        # context의 각 문서를 문자열로 결합\n",
    "        context_text = \"\\n\".join([doc.page_content for doc in inputs[\"context\"]])\n",
    "        return {\"context\": context_text, \"question\": inputs[\"question\"]}\n",
    "\n",
    "# RAG 체인에서 각 단계마다 DebugPassThrough 추가\n",
    "rag_chain_debug = {\n",
    "    \"context\": retriever,                    # 컨텍스트를 가져오는 retriever\n",
    "    \"question\": DebugPassThrough()        # 사용자 질문이 그대로 전달되는지 확인하는 passthrough\n",
    "}  | DebugPassThrough() | ContextToText() | prompt_test | model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. 챗봇 구동 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================\n",
      "Debug Output: 카나나는 카카오의 AI서비스이다\n",
      "Debug Output: {'context': [Document(metadata={'source': 'files/인공지능산업최신동향_2024년_11월호.pdf', 'page': 13}, page_content='등의 단어를 조합한 카나나는 ‘가장 나다운 AI’를 의미∙카카오는 동 브랜드를 자사가 개발하는 주요 AI 모델과 신규 서비스의 이름에 두루 사용할 계획으로, AI 메이트 서비스')], 'question': '카나나는 카카오의 AI서비스이다'}\n",
      "Final Response:\n",
      "It seems that the user question is missing. Please provide the user question so I can assess the relevance of the retrieved documents accordingly.\n",
      "<class 'str'>\n",
      "========================\n",
      "Debug Output: 미국의 발표 내용을 설명해줘\n",
      "Debug Output: {'context': [Document(metadata={'source': 'files/인공지능산업최신동향_2024년_11월호.pdf', 'page': 15}, page_content='£국무부, AI 연구 우선순위로 포괄적 연구 인프라 조성과 글로벌 도전과제 해결 등 제시n미국 국무부(United States Department of State)가 2024년')], 'question': '미국의 발표 내용을 설명해줘'}\n",
      "Final Response:\n",
      "It seems that the user question is missing. Could you please provide the user question along with the retrieved documents? This will allow me to assess the relevance of the documents accordingly.\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "while i<2:\n",
    "\tprint(\"========================\")\n",
    "\tquery = input(\"질문을 입력하세요: \")\n",
    "\tresponse = rag_chain_debug.invoke(query)\n",
    "\tprint(\"Final Response:\")\n",
    "\tprint(response.content)\n",
    "\tprint(type(response.content))\n",
    "\ti += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
