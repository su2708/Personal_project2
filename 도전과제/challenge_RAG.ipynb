{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ch3 LLM, RAG 개인 도전과제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal: LangSmith의 Prompt Library를 참고하여 prompt engineering을 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 사용환경 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"True\"  # 라이브러리끼리의 충돌을 해결\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"OpenAI API key 입력: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. 모델 로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 모델 초기화\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. 문서 로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# PDF 파일 로드. 파일의 경로 입력\n",
    "loader = PyPDFLoader(\"files/인공지능산업최신동향_2024년_11월호.pdf\")\n",
    "\n",
    "# 페이지 별 문서 로드\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. 문서 청크로 나누기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RecursiveCharacterTextSplitter**\n",
    "- 주어진 문자 목록의 순서대로 청크가 충분히 작아질 때까지 재귀적으로 텍스트를 분할하는 클래스\n",
    "- 문자 목록을 매개변수로 받아 동작\n",
    "- 기본 문자 목록: \\[\"\\n\\n\", \"\\n\", \" \", \"\"\\] (default)\n",
    "- 텍스트를 재귀적으로 분할하여 의미적으로 관련있는 텍스트 조각들이 같이 있도록 하는 목적으로 설계됨\n",
    "\n",
    "---\n",
    "- `chunk_size`: 각 청크의 최대 길이\n",
    "- `chunk_overlap`: 인접한 청크 사이에 중복으로 포함될 문자의 수\n",
    "- `length_function`: 청크의 길이를 계산하는 함수\n",
    "- `is_separator_regex`: 구분자로 정규식을 사용할지 여부를 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "splits = recursive_text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. 벡터 임베딩 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# OpenAI 임베딩 모델 초기화\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. 벡터 스토어 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7. FAISS를 Retriever로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8. 프롬프트 템플릿 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3_envs\\SpartaProjects\\Personal_Project\\project2\\Lib\\site-packages\\langsmith\\client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "d:\\Anaconda3_envs\\SpartaProjects\\Personal_Project\\project2\\Lib\\site-packages\\langsmith\\client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n",
      "d:\\Anaconda3_envs\\SpartaProjects\\Personal_Project\\project2\\Lib\\site-packages\\langsmith\\client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# LangChain library에서 prompt, model, chain, agent 등을 불러오는 라이브러리\n",
    "from langchain import hub\n",
    "\n",
    "# Prompts 디렉토리가 없다면 Prompts 디렉토리 만들기\n",
    "os.makedirs(\"Prompts\", exist_ok=True)\n",
    "\n",
    "# LangChain library에서 prompt 가져오기\n",
    "prompt1 = hub.pull(\"rlm/rag-prompt\")\n",
    "prompt2 = hub.pull(\"rlm/rag-document-relevance\")\n",
    "prompt3 = hub.pull(\"rlm/rag-answer-hallucination\")\n",
    "\n",
    "# 가져온 prompt들을 prompts list에 저장\n",
    "prompts = [prompt1, prompt2, prompt3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompts 디렉토리에 prompt 저장\n",
    "for i in range(1, 4):\n",
    "    # 파일 경로를 .\\Prompts\\prompt.txt 형태로 설정\n",
    "    file_path = os.path.join(\".\\Prompts\", f\"prompt{i}.txt\")\n",
    "    \n",
    "    # ChatPromptTemplate을 str 타입으로 변환\n",
    "    prompt = prompts[i-1].format(question=\"question\", context=\"context\")\n",
    "    \n",
    "    \n",
    "    with open(file_path, 'w') as f:\n",
    "        f.write(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: question \n",
      "Context: context \n",
      "Answer: \n",
      "\n",
      "\n",
      "System: You are a grader assessing relevance of a retrieved document to a user question.\n",
      "\n",
      "If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\n",
      "\n",
      "It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n\n",
      "\n",
      "Give a binary score 1 or 0 score, where 1 means that the document is relevant to the question.\n",
      "Human: Retrieved documents:  \n",
      "\n",
      "User question:  \n",
      "\n",
      "\n",
      "System: You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \n",
      "\n",
      "Give a binary score 1 or 0, where 1 means that the answer is grounded in / supported by the set of facts.\n",
      "Human: Facts:  \n",
      "\n",
      "LLM generation:  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompts = {}\n",
    "for i in range(1, 4):\n",
    "    file_path = os.path.join(\".\\Prompts\", f\"prompt{i}.txt\")\n",
    "    with open(file_path, 'r') as f:\n",
    "        prompts[f\"prompt{i}\"] = f.read()\n",
    "        \n",
    "for i in range(1, 4):\n",
    "    print(prompts[f\"prompt{i}\"], \"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "contextual_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the question using only the following context.\"),\n",
    "    (\"user\", \"Context: {context}\\\\n\\\\nQuestion: {question}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9. RAG 체인 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "class DebugPassThrough(RunnablePassthrough):\n",
    "    def invoke(self, *args, **kwargs):\n",
    "        output = super().invoke(*args, **kwargs)\n",
    "        print(\"Debug Output:\", output)\n",
    "        return output\n",
    "# 문서 리스트를 텍스트로 변환하는 단계 추가\n",
    "class ContextToText(RunnablePassthrough):\n",
    "    def invoke(self, inputs, config=None, **kwargs):  # config 인수 추가\n",
    "        # context의 각 문서를 문자열로 결합\n",
    "        context_text = \"\\n\".join([doc.page_content for doc in inputs[\"context\"]])\n",
    "        return {\"context\": context_text, \"question\": inputs[\"question\"]}\n",
    "\n",
    "# RAG 체인에서 각 단계마다 DebugPassThrough 추가\n",
    "rag_chain_debug = {\n",
    "    \"context\": retriever,                    # 컨텍스트를 가져오는 retriever\n",
    "    \"question\": DebugPassThrough()        # 사용자 질문이 그대로 전달되는지 확인하는 passthrough\n",
    "}  | DebugPassThrough() | ContextToText() | contextual_prompt | model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 10. 챗봇 구동 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================\n",
      "Debug Output: \n",
      "Debug Output: {'context': [Document(metadata={'source': 'files/인공지능산업최신동향_2024년_11월호.pdf', 'page': 1}, page_content='·····························21')], 'question': ''}\n",
      "Final Response:\n",
      "It seems that there is no specific question provided. Could you please provide more details or clarify your question?\n",
      "========================\n",
      "Debug Output: \n",
      "Debug Output: {'context': [Document(metadata={'source': 'files/인공지능산업최신동향_2024년_11월호.pdf', 'page': 1}, page_content='·····························21')], 'question': ''}\n",
      "Final Response:\n",
      "It seems that the question is incomplete or not provided. Please provide the full question for me to assist you effectively.\n",
      "========================\n",
      "Debug Output: \n",
      "Debug Output: {'context': [Document(metadata={'source': 'files/인공지능산업최신동향_2024년_11월호.pdf', 'page': 1}, page_content='·····························21')], 'question': ''}\n",
      "Final Response:\n",
      "It seems that the context provided is incomplete and does not contain sufficient information to address any specific question. Please provide more details or a clearer question so I can assist you effectively.\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "while i<3:\n",
    "\tprint(\"========================\")\n",
    "\tquery = input(\"질문을 입력하세요: \")\n",
    "\tresponse = rag_chain_debug.invoke(query)\n",
    "\tprint(\"Final Response:\")\n",
    "\tprint(response.content)\n",
    "\ti += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================\n",
      "query: \n",
      "Final Response:\n",
      "Hello! How can I assist you today?\n",
      "========================\n",
      "query: \n",
      "Final Response:\n",
      "Hello! How can I assist you today?\n",
      "========================\n",
      "query: \n",
      "Final Response:\n",
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# 일반 ChatGPT와 답변 비교\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"========================\")\n",
    "    query = input(\"질문을 입력하세요: \")\n",
    "    print(f\"query: {query}\")\n",
    "    response = llm.invoke(query)\n",
    "    print(\"Final Response:\")\n",
    "    print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***RAG VS 일반 GPT***\n",
    "\n",
    "1. 정보 접근\n",
    "- 일반 GPT 모델\n",
    "    - 사전에 학습된 데이터에 의존하여 대답\n",
    "    - 사전 학습 데이터 이후의 정보 혹은 학습된 지식을 뛰어 넘는 정보에 대한 질문은 답변하기 어려움\n",
    "- RAG 모델\n",
    "    - 외부 검색 시스템이나 데이터베이스로부터 <span style=\"color:lime\">필요한 정보를 검색</span>하여 응답 생성에 활용\n",
    "    - 일반 GPT 모델보다 최신 정보를 제공하기에 알맞음\n",
    "\n",
    "2. 데이터 출처의 신뢰성\n",
    "- 일반 GPT 모델\n",
    "    - 학습된 정보를 바탕으로 추론을 하므로, 정보의 출처를 명확히 제시할 수 없고 특정한 사실에 대해 신뢰도가 떨어질 수 있음\n",
    "- RAG 모델\n",
    "    - 필요한 정보를 검색하는 과정을 거치므로, 정보의 출처를 확인할 수 있고 검색된 자료의 신뢰도를 판단할 수 있음\n",
    "\n",
    "3. 학습된 지식 업데이트\n",
    "- 일반 GPT 모델\n",
    "    - 모델을 다시 훈련하지 않는 이상 지식이 업데이트 되지 않음\n",
    "- RAG 모델\n",
    "    - 새로운 정보를 검색함으로써 지식의 업데이트 없이도 실시간 데이터에 접근 가능\n",
    "\n",
    "4. 요약\n",
    "- **최신 정보나 특정 지식을 필요로 하는 경우, 한정된 지식으로 인해 생길 수 있는 문제를 해소하고 보다 신뢰성 높은 정보를 제공하기 위해 RAG 모델이 필요**\n",
    "- RAG 모델은 카카오의 신규 AI 서비스인 카나나에 대해 잘 설명한 반면, 일반 모델은 그렇지 못함"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
